训练过程
>借鉴各位大佬的，自己对代码和模型知道的还是太少了

* [数据处理](#数据处理)
* [训练单模型](#训练单模型)
* [KNN](#KNN)
* [Clip](#Clip)
* [模型集成](#模型集成)


## 数据处理
首先是对diffusiondb-2m的数据进行筛选，（1）筛选仅英文，（2）筛选图片尺寸长宽为512✖️512的图片，（3）prompt长度大于5，（4）没有特殊符号的，（5）头和尾各15个字符相同的prompt删除重复的，也可以用cossimilarity>0.8（可以自己调整阈值）来删除相似的prompt。最终得到一拥有462636条prompt和对应图片地址的data。

利用他人产生的图片prompt配对data，大多是用chatgpt组合产生出prompt，再用sd2生成图片。

最终使用的数据集样本量为576668。主要也是因为计算资源有限，kaggle的免费计算资源承受不了再大的数据集跑模型了。


## 训练单模型
#### [Vit]()
(1)vit_base_patch16_224 -> vit_large_patch16_384 单模型效果提升

#### [Clip-Vit]()
[clip-vit-large-patch14](https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/398529)

非常巧妙地训练模型达到0.58+的一种方法，我也是用这个单模型训练57万数据达到了我的单模型最高lb：0.55835

(1)从 SDDB2m 数据集开始，根据提示语句嵌入的相似性对其进行过滤。我删除了与其他嵌入相关性 >0.95 的图像/提示对，导致图像少于 500K。
(2)同时下载30K和80K图像数据集。取出一些（比如25K不高度相关的）图像作为测试集。
(3)加载预训练的 openai 剪辑模型（或您选择的任何其他足够强大的模型），解冻几个变换器层，训练模型（稍后将共享代码）。
(4)生成您自己的样本，使用在句子嵌入（来自测试数据）上训练的模型来预测预测和实际数据的余弦相似度。这可以帮助您决定生成哪些图像以关注主模型的薄弱区域。
(5)硬编码提示生成器（将共享）并使用它来生成提示候选人。
(6)使用提示微调 GPT2 模型并将其用作提示生成器（将分享）。
(7)根据预测的相似性过滤来自步骤 5 和 6 的提示（例如，仅包括预测相似性 <0.4 的样本）。此外，根据生成的提示和现有提示之间的嵌入相似性（例如，>0.5）过滤它们，使它们更像提示。这样，您将获得类似于提示的样本，但模型很难根据图像进行预测。
(8)将生成的图像划分为训练集和测试集，扩展这两个数据集。再次训练主模型，必要时解冻更多层。
(9)使用更新的测试数据和模型训练新的相似性预测模型。
(10)迭代（时间允许多少次，^_^）步骤 7、8 和 9 以获得更多样本并改进模型预测。

## KNN
看似平常的KNN方法，也能得到0.5513的score

## ClipInterrogator

## 模型集成
(1)CLIPInterrogator+OFA+ViT
(2)KNN+CLIPInterrogator+Clip-Vit



